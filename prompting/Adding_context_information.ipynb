{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/littleadam/AI_ML/blob/main/prompting/Adding_context_information.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysy--KfNRrCq"
      },
      "source": [
        "While LLMs are trained extensively on various documents and data, the LLM does not know everything. New information or information that is not easily accessible cannot be known by the LLM, unless it was specifically added to its corpus of knowledge somehow. For this reason, it is sometimes necessary to provide the LLM, with information and context necessary to answer our queries by providing additional context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ne-3gnXqR0hI",
        "outputId": "833076e6-e3db-466d-91bd-86e81fdfd7d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/153.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.4/153.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/760.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EconMHePQHGw"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eomJzCa6lb90"
      },
      "source": [
        "## Configure your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v-JZzORUpVR2"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JljcHgI2ltTY"
      },
      "source": [
        "## Example\n",
        "\n",
        "Let's say you provide some statistics from a recent Olympics competition, and this data wasn't used to train the LLM. Insert it into the prompt, and input the prompt to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uFcm6Dd7ls_F",
        "outputId": "87d2f931-4556-4e03-c2d1-5b25ddd69582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The following athletes competed in the Olympics exactly 9 times:\n\n* **Hubert Raudaschl**\n* **Afanasijs Kuzmins**\n* **Nino Salukvadze** \n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# the list as of April 2024\n",
        "prompt = \"\"\"\n",
        "QUERY: provide a list of atheletes that competed in olympics exactly 9 times.\n",
        "CONTEXT:\n",
        "\n",
        "Table title: Olympic athletes and number of times they've competed\n",
        "Ian Millar, 10\n",
        "Hubert Raudaschl, 9\n",
        "Afanasijs Kuzmins, 9\n",
        "Nino Salukvadze, 9\n",
        "Piero d'Inzeo, 8\n",
        "Raimondo d'Inzeo, 8\n",
        "Claudia Pechstein, 8\n",
        "Jaqueline Mourão, 8\n",
        "Ivan Osiier, 7\n",
        "François Lafortune, Jr, 7\n",
        "\n",
        "\"\"\"\n",
        "model = genai.GenerativeModel(model_name='gemini-1.5-flash-latest')\n",
        "Markdown(model.generate_content(prompt).text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1jF9yUx91Fv"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "While some information may be easily searchable online without the use of an LLM, consider data that is not found on the internet, such as private documentation, quickbooks, and forums. Use this code as a template to help you input that information into the Gemini model.\n",
        "\n",
        "Be sure to explore other examples of prompting in the repository. Try writing prompts about classifying your own data, or try some of the other prompting techniques such as few-shot prompting."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Adding_context_information.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}