{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw1C15ZHYsAPgeWqBDI7/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/littleadam/AI_ML/blob/main/Testing_semantic_search_using_the_Pinecone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUW1dB_JYjpT"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone-client\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/sampled_reviews.csv')\n",
        "\n",
        "# Sample 2000 reviews from the dataset\n",
        "sampled_reviews = df.sample(2000, random_state=2)\n",
        "\n",
        "# Extract relevant columns (Review Text and Summary can be used for semantic search)\n",
        "sampled_reviews = sampled_reviews[['Text', 'Summary']]\n",
        "\n",
        "# Save the sampled data for later use\n",
        "sampled_reviews.to_csv('sampled_amazon_food_reviews.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy pinecone gemini\n"
      ],
      "metadata": {
        "id": "ZQwtRgH9o8cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install protoc_gen_openapiv2"
      ],
      "metadata": {
        "id": "GWtJakiuuWiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain\n",
        "#!pip install --upgrade protobuf\n",
        "#!pip install --upgrade google-api-python-client google-auth google-auth-httplib2 google-auth-oauthlib\n",
        "#!pip install protobuf==3.20.*\n",
        "\n",
        "#!pip install pinecone\n",
        "!pip install protoc_gen_openapiv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pinecone\n",
        "import os\n",
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "import google.generativeai as genai\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyD0Y-aGZ_BvwyqTzQQnlQNtLafLQC3WUhE\")\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "pc = Pinecone(api_key=\"24df3a07-00c4-4dea-bd4d-cbb487ec1946\")\n",
        "\n",
        "#if \"reviews-text\" not in pc.list_indexes().names():\n",
        "#  pc.create_index(\n",
        "#  name=\"review-text\",\n",
        "#  dimension=1536,\n",
        "#  metric=\"cosine\",\n",
        "#  spec=ServerlessSpec(\n",
        "#    cloud=\"aws\",\n",
        "#    region=\"us-east-1\"\n",
        "#  ),\n",
        "#  deletion_protection=\"disabled\"\n",
        "#  )\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/Reviews.csv\")\n",
        "\n",
        "# Sample 2000 reviews randomly\n",
        "sampled_data = data.sample(10)#(2000)\n",
        "#print(sampled_data.columns)\n",
        "\n",
        "# Function to generate embeddings using Gemini\n",
        "def generate_embeddings(texts):\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for text in texts:\n",
        "        #print(f\"the input is : {text}\")\n",
        "        #inputs = tokenizer({str(text)}, return_tensors=\"pt\")\n",
        "        inputs = tokenizer(\"Do not buy this from amazon. get it at sam's club for around $6.00 for the case shown. Buy from sam's club before choosing amazon.\",return_tensors=\"pt\")\n",
        "        #print(inputs)\n",
        "        outputs = model(**inputs)\n",
        "        #embeddings.append(outputs)\n",
        "        #print(f\"the text is {text}\")\n",
        "        #query=f\"Generate an embedding for the following text: {text}\"\n",
        "        #response = model.generate_content(query)\n",
        "        #print(f\"\\nthe response text is {response.text}\\n\")\n",
        "\n",
        "        #embeddings.append(response.text.strip())\n",
        "    return embeddings\n",
        "\n",
        "print(sampled_data[\"Text\"])\n",
        "# Generate embeddings for the sampled reviews\n",
        "embeddings = generate_embeddings(sampled_data[\"Text\"])\n",
        "\n",
        "# Upsert vectors to the index\n",
        "vectors = [\n",
        "    (str(i), embedding) for i, embedding in enumerate(embeddings)\n",
        "]\n",
        "\n",
        "print(f\"Vectors content \\n:{vectors}\")\n",
        "index = pc.Index(\"review-text\")\n",
        "print(index)\n",
        "index_name = \"reviews-text\"\n",
        "print(index_name)\n",
        "#index.upsert(index_name=index_name, vectors=vectors)\n",
        "\n",
        "# Example of semantic search\n",
        "query = \"I'm looking for a spicy Indian curry.\"\n",
        "#query_embedding = generate_embeddings([query])[0]\n",
        "#results = pinecone.query(\n",
        "#    index_name=index_name,\n",
        "#    vector=query_embedding,\n",
        "#    top_k=5,\n",
        "#)\n",
        "\n",
        "#print(results)"
      ],
      "metadata": {
        "id": "Py4oqp-3pzvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch"
      ],
      "metadata": {
        "id": "IXU0iuuTqRKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac80b18-0c47-4de6-b534-6227fb7245aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "sampled_data = \"\"\n",
        "\n",
        "#def data_process():\n",
        "  # Load the dataset\n",
        "data = pd.read_csv(\"/content/Reviews.csv\")\n",
        "\n",
        "  # Sample 2000 reviews randomly\n",
        "sampled_data = data.sample(10)#(2000)\n",
        "\n",
        "# Function to get BERT embeddings for a given string\n",
        "def get_bert_embeddings(text):\n",
        "    # Load pre-trained BERT model and tokenizer\n",
        "    model_name = 'bert-base-uncased'\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize the input text and convert it into input IDs and attention mask\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Get the model outputs (embeddings)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # The embeddings for each token (we will take the last hidden state)\n",
        "    embeddings = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
        "\n",
        "    # To get a single embedding for the entire sentence, you can take the average of the token embeddings\n",
        "    sentence_embedding = torch.mean(embeddings, dim=1)\n",
        "\n",
        "    return sentence_embedding\n",
        "\n",
        "def generate_embeddings(texts_data):\n",
        "\n",
        "    embeddings = []\n",
        "    for text in texts_data:\n",
        "      response = get_bert_embeddings({text})\n",
        "      embeddings.append(response.text.strip())\n",
        "\n",
        "    return(embeddings)\n",
        "\n",
        "print(sampled_data[\"Text\"])\n",
        "# Example usage\n",
        "text = str(\"BERT is a powerful model for NLP tasks.\")\n",
        "get_bert_embeddings(text)\n",
        "embeddings = generate_embeddings(\"BERT is a powerful model for NLP tasks.\")\n",
        "#embeddings = generate_embeddings(sampled_data[\"Text\"])\n",
        "\n",
        "# Print the shape of the embedding\n",
        "#print(f\"Embedding shape: {embeddings.shape}\")\n",
        "print(f\"Embedding: {embeddings}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "Q7ZET3gX_c1G",
        "outputId": "8215e15d-8567-4598-df2e-31a7898b1dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "438701    We ordered this package of 35 because it had t...\n",
            "548074    I give myself a Keratin Straightening Treatmen...\n",
            "59902     These were a decent value purchased at $6.99. ...\n",
            "61699     Compared to the Finnish and Swedish one's I've...\n",
            "74568     My son is a 50plus lover of Apple Jacks. It wa...\n",
            "495175    Ordered six \"hairball formula\" bags of cat foo...\n",
            "67889     For those of you who, like me, had to order th...\n",
            "219394    I can't say enough good things about the Espre...\n",
            "57066     I first tried this k-cup as a sample.  I was h...\n",
            "9518      I love this product! BUT the downside is that ...\n",
            "Name: Text, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8e5dbce19cec>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BERT is a powerful model for NLP tasks.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mget_bert_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BERT is a powerful model for NLP tasks.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;31m#embeddings = generate_embeddings(sampled_data[\"Text\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-8e5dbce19cec>\u001b[0m in \u001b[0;36mgenerate_embeddings\u001b[0;34m(texts_data)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bert_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-8e5dbce19cec>\u001b[0m in \u001b[0;36mget_bert_embeddings\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Tokenize the input text and convert it into input IDs and attention mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3053\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3055\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3056\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3057\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3115\u001b[0m                 \u001b[0;34m\"text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3116\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "sampled_data = \"\"\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/Reviews.csv\", index=False)\n",
        "\n",
        "# Sample 2000 reviews randomly\n",
        "sampled_data = data.sample(10)#(2000)\n",
        "#print(sampled_data)\n",
        "# Function to get BERT embeddings for a given string\n",
        "def get_bert_embeddings(text):\n",
        "    # Load pre-trained BERT model and tokenizer\n",
        "    model_name = 'bert-base-uncased'\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize the input text and convert it into input IDs and attention mask\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Get the model outputs (embeddings)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # The embeddings for each token (we will take the last hidden state)\n",
        "    embeddings = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
        "\n",
        "    # To get a single embedding for the entire sentence, you can take the average of the token embeddings\n",
        "    sentence_embedding = torch.mean(embeddings, dim=1)\n",
        "\n",
        "    return sentence_embedding\n",
        "\n",
        "# Example usage\n",
        "text = \"BERT is a powerful model for NLP tasks.\"\n",
        "#second_text_column  = sampled_data.iloc[:, sampled_data.columns.get_loc('Text') + 1]\n",
        "#print(second_text_column)\n",
        "print(sampled_data['Text'])\n",
        "embedding = get_bert_embeddings(sampled_data['Text'])\n",
        "\n",
        "# Print the shape of the embedding\n",
        "print(f\"Embedding shape: {embedding.shape}\")\n",
        "print(f\"Embedding: {embedding}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "YkPIkd7K_h31",
        "outputId": "dc59b231-9410-48da-fa81-a8dc452eef63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "read_csv() got an unexpected keyword argument 'index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4ed6ff525cdd>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Reviews.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Sample 2000 reviews randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'index'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pkg4oCD1Dwwk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}