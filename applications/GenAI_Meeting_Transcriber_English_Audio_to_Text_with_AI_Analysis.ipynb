{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Notes: Remember to ALWAYS mount drive!"
      ],
      "metadata": {
        "id": "JYDgRPzp9g8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "w1UcKaKsj8cC",
        "outputId": "83d0c174-e761-4133-936f-f6bb7bfc6c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.51.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n"
      ],
      "metadata": {
        "id": "1zD7qlVnmd7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWSJpsyKqHjH",
        "outputId": "538ca5e4-5081-4236-806e-d42e5a1fe0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = 'c2staUFBT3BKZWxsTFlrZjlSeGJIc0RUM0JsYmtGSjVMTGY3ZlFSVEc0QmxJUm0walF6'\n",
        "\n",
        "\n",
        "# Your Base64 encoded string\n",
        "encoded_str = API_KEY  # This is 'Hello World!' in Base64\n",
        "\n",
        "# Decode the Base64 encoded string\n",
        "decoded_bytes = base64.b64decode(encoded_str)\n",
        "\n",
        "# Convert the bytes object to a string\n",
        "decoded_str = decoded_bytes.decode('utf-8')\n",
        "\n",
        "#print(decoded_str)\n",
        "client = OpenAI(api_key=decoded_str)\n"
      ],
      "metadata": {
        "id": "PHQ0jRIPmIZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_text(text, line_length=85):\n",
        "    text = str(text)\n",
        "    words = text.split()\n",
        "    formatted_text = \"\"\n",
        "    current_line = \"\"\n",
        "\n",
        "    for word in words:\n",
        "        if len(current_line) + len(word) + 1 <= line_length:\n",
        "            current_line += (word + \" \")\n",
        "        else:\n",
        "            formatted_text += current_line.rstrip() + \"\\n\"\n",
        "            current_line = word + \" \"\n",
        "\n",
        "    # Add the last line if it's not empty\n",
        "    if current_line:\n",
        "        formatted_text += current_line.rstrip()\n",
        "\n",
        "    return formatted_text"
      ],
      "metadata": {
        "id": "6fv73pX7KO5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_response\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {decoded_str}\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n",
        "    \"temperature\": 0.7\n",
        "}\n",
        "\n",
        "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=data)\n",
        "print(format_text(response.json()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL1Ganlbl9S-",
        "outputId": "4512674f-aaab-47a7-e69c-bf064f10d221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-8o7Ah45PA7cZ61WAb8l6FYw595YPT', 'object': 'chat.completion',\n",
            "'created': 1706954659, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0,\n",
            "'message': {'role': 'assistant', 'content': 'This is a test!'}, 'logprobs': None,\n",
            "'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 5,\n",
            "'total_tokens': 18}, 'system_fingerprint': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#audio transcription\n",
        "\n",
        "audio_file= open(\"/content/drive/MyDrive/HCL Project/Gen AI ML /Capstone/Capstone hcl/videoplayback.mp3\", \"rb\")\n",
        "transcript = client.audio.transcriptions.create(\n",
        "  model=\"whisper-1\",\n",
        "  file=audio_file\n",
        ")\n"
      ],
      "metadata": {
        "id": "tgnapIfu0-fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_text((transcript)))\n",
        "print('\\n')\n",
        "print(type(transcript))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTV0VAlXI-8f",
        "outputId": "2f145627-474b-4d1a-f78f-17b60864aca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription(text=\"Hello Oh, well I have a youth and everybody boy with That's\n",
            "right. They don't say is what happened here. Did you hear today? Yes, he's here\n",
            "today How do you think his chance was there any chance to get a little talking\n",
            "picture of him We got all our sound equipment talking pick. Yeah, get all the sound\n",
            "equipment I hear in the truck bring it bring the stuff in Anthony right in here.\n",
            "Hurry up. You want to talk to you? Yeah, we want to get a talk He has a very feeble\n",
            "voice and moreover he has a very deep prejudice against being photographed at all If\n",
            "England does not grant your demand, what force of action will you follow them? Of\n",
            "course civil disobedience And all other phases of the Tiagra are always at our\n",
            "disposal But whether we shall resort to this weapon immediately or what other steps\n",
            "we shall take, I cannot judge at present If England grants your demands, Mr. Gandhi,\n",
            "do you intend to have complete prohibition in the new Indian states? Oh, yes\n",
            "Absolute prohibition? Absolute And do you intend also, if India wins its\n",
            "independence, to abolish child marriages? I should very much like to, even before Do\n",
            "you expect England this time will give India full self-government? That also is more\n",
            "than I can say But you are hopeful I am an optimist You are an optimist If you go to\n",
            "the second round-table conference, will you go attired in native Indian dress or\n",
            "will you prefer European dress? I should certainly not be found in European dress\n",
            "And if the weather permitted, I should present myself exactly as I am today And if\n",
            "the King of England invited you to dinner at Buckingham Palace, you would go in your\n",
            "customary Indian dress? In any other dress, I should be most discreet to him,\n",
            "because I should be artificial If England does not grant your demands, are you\n",
            "prepared to return to jail again? I am always prepared to return to jail Would you\n",
            "be prepared to die in the cause of India's independence? It is a bad question\")\n",
            "\n",
            "\n",
            "<class 'openai.types.audio.transcription.Transcription'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transcript Converter\n",
        "\n",
        "response = client.completions.create(\n",
        "  model=\"gpt-3.5-turbo-instruct\",\n",
        "  prompt=\"Write meeting minutes from the transcription \\n\" + format_text(transcript)\n",
        ")"
      ],
      "metadata": {
        "id": "fWeQfM2WKheb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-0125\",\n",
        "  #response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to provide meeting minutes from a given transcript. \\\n",
        "                                  Please analyze the following transcript and create detailed meeting minutes. \\\n",
        "                                  Ensure to include sections such as date, location, attendees, agenda, detailed points of discussion, action items, and next meeting details. \\\n",
        "                                  Here is the transcript: \\n\\n\" + format_text(transcript)},\n",
        "    {\"role\": \"assistant\", \"content\": \"Is there anything else you would like to know or any other assistance you need regarding this topic or something else?\"}\n",
        "    {\"role\": \"user\", \"content\": input()}\n",
        "  ]\n",
        ")\n",
        "response = format_text(response.choices[0].message.content)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ttCXbcETdyxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Add [text streaming](https://platform.openai.com/docs/api-reference/streaming) to response smoother chat experience"
      ],
      "metadata": {
        "id": "mNHOh0zTyHn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-0125\",\n",
        "  #response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provided meeting minutes from a given transcript earlier. \\\n",
        "                                  Please provide any follow up help to the user. \\\n",
        "                                  Here is the meeting minutes from before: \\n\\n\" + format_text(response)},\n",
        "    {\"role\": \"assistant\", \"content\": input(\"Is there anything else you would like to know or any other assistance you need regarding this topic or something else?\\n\")}\n",
        "  ]\n",
        ")\n",
        "response = format_text(user_response.choices[0].message.content)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wWgHIojyeMt",
        "outputId": "5ceb2fbd-5ef2-44c6-f3c9-5d7be7342e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there anything else you would like to know or any other assistance you need regarding this topic or something else?\n",
            "Hmm, how long would you estimate was this meeting\n",
            "As the meeting likely took place during the late 19th century to 1947, it's safe to\n",
            "assume that the duration of the meeting could have been anywhere from a few minutes\n",
            "to several hours, depending on the importance of the topic being discussed and the\n",
            "participants' schedules. If you need any more information or have any other\n",
            "questions, feel free to let me know!\n"
          ]
        }
      ]
    }
  ]
}